{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SimpleEnv, self).__init__()\n",
    "        \n",
    "        # Define action space: The agent can choose between 0 or 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Define observation space: The state is just one number\n",
    "        self.observation_space = spaces.Box(low=0, high=10, shape=(1,))\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.state = 5  # Starting state\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = 5\n",
    "        return np.array([self.state])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (new_state, reward, done, info).\"\"\"\n",
    "        if action == 0:  # If action is 0, decrease the state\n",
    "            self.state -= 1\n",
    "        elif action == 1:  # If action is 1, increase the state\n",
    "            self.state += 1\n",
    "        \n",
    "        # Reward: High reward for being close to 10\n",
    "        reward = 10 - abs(self.state - 10)\n",
    "        \n",
    "        # Done: Episode ends if state goes out of bounds\n",
    "        done = self.state < 0 or self.state > 20\n",
    "        \n",
    "        return np.array([self.state]), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [5], Reward: 5, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [1], Reward: 1, Done: False\n",
      "State: [0], Reward: 0, Done: False\n",
      "State: [-1], Reward: -1, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Test the Simple Environment\n",
    "env = SimpleEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Choose a random action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the Portfolio Environment\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data: np.ndarray, initial_cash: float = 10000.0, transaction_fee: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize the portfolio environment.\n",
    "        :param data: Historical asset price data (numpy array).\n",
    "        :param initial_cash: Starting capital.\n",
    "        :param transaction_fee: Transaction fee percentage.\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.num_assets = data.shape[1]\n",
    "        self.transaction_fee = transaction_fee\n",
    "        \n",
    "        # Define state space and action space\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_assets + 2,))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,))\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.portfolio = np.zeros(self.num_assets)\n",
    "        logger.info(f\"Environment reset. Initial cash: {self.cash}, Portfolio: {self.portfolio}\")\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        :param action: Allocation proportions for each asset.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Step {self.current_step}: Taking action: {action}\")\n",
    "        \n",
    "        # Normalize action to ensure it sums to 1\n",
    "        action = action / (np.sum(action) + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Calculate portfolio rebalancing\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices) + self.cash\n",
    "        new_portfolio = portfolio_value * action / current_prices\n",
    "        transaction_costs = np.sum(np.abs(new_portfolio - self.portfolio)) * self.transaction_fee  # Transaction fee\n",
    "        reward = portfolio_value - transaction_costs - self.cash\n",
    "        \n",
    "        # Update state\n",
    "        self.cash = portfolio_value - np.sum(new_portfolio * current_prices)\n",
    "        self.portfolio = new_portfolio\n",
    "        logger.info(f\"New portfolio: {self.portfolio}, Remaining cash: {self.cash}, Reward: {reward}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Generate the current observation.\n",
    "        \"\"\"\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices)\n",
    "        return np.concatenate([current_prices, [portfolio_value, self.cash]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0. 0. 0.]\n",
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0. 0. 0.]\n",
      "Step 0: Taking action: [0.8586781  0.43614116 0.3359825  0.5037337  0.93673825]\n",
      "New portfolio: [31.96922775  9.78873787  8.87952225 14.92862387 46.49257334], Remaining cash: -0.0005960464477539062, Reward: -0.11205868509023276\n",
      "Step 1: Taking action: [0.77974564 0.9315476  0.103168   0.04839943 0.7945262 ]\n",
      "New portfolio: [49.72627402 69.8295044   3.15915911  1.83882639 27.51366326], Remaining cash: -0.00018636338973010425, Reward: 11116.894280713914\n",
      "Step 2: Taking action: [0.12539876 0.194407   0.9603254  0.87029105 0.50164866]\n",
      "New portfolio: [13.87920529  7.62050564 41.52718681 70.39475779 42.39253455], Remaining cash: -0.0006831075424997834, Reward: 15280.637392113797\n",
      "Step 3: Taking action: [0.12776478 0.66636235 0.12653199 0.7538569  0.95374167]\n",
      "New portfolio: [11.19016303 49.5937177   7.39065272 48.4174331  72.14921539], Remaining cash: 0.0004688362514571054, Reward: 15731.404299885402\n",
      "Step 4: Taking action: [0.30270243 0.14904235 0.17254128 0.18727848 0.9521022 ]\n",
      "New portfolio: [24.84335916 21.26743222 19.87605809 19.72560414 90.87315463], Remaining cash: 0.0007194483860075707, Reward: 16093.68561597904\n",
      "Step 5: Taking action: [0.90337896 0.09870115 0.37272996 0.40234184 0.5487784 ]\n",
      "New portfolio: [41.75761065  8.3802096  21.83153275 21.87942876 59.65880145], Remaining cash: -0.0013383151490415912, Reward: 13817.28942908259\n",
      "Step 6: Taking action: [0.28388748 0.33009115 0.92926294 0.4945941  0.21043381]\n",
      "New portfolio: [ 20.90252994  40.1451802  134.1110117   27.83740667  11.70858763], Remaining cash: -0.0012294047592149582, Reward: 18333.995202329377\n",
      "Step 7: Taking action: [0.33160487 0.89114404 0.5130291  0.03358749 0.56568897]\n",
      "New portfolio: [19.94705325 87.16816939 67.55795772  2.23222447 47.35620689], Remaining cash: -0.0004963552528352011, Reward: 18377.666773907506\n",
      "Step 8: Taking action: [0.32436803 0.22197166 0.58863896 0.23861305 0.78368235]\n",
      "New portfolio: [42.14094656 18.02522049 89.01746749 13.68257414 83.46560425], Remaining cash: -0.0003896727394021582, Reward: 17433.503409923334\n",
      "End of the episode reached.\n"
     ]
    }
   ],
   "source": [
    "# Simulating Sample Data\n",
    "np.random.seed(42)\n",
    "sample_data = np.random.uniform(low=50, high=150, size=(10, 5))  # 5 assets, 10 timesteps\n",
    "\n",
    "# Running the environment\n",
    "env = PortfolioEnv(data=sample_data, initial_cash=10000.0)\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(len(sample_data)):\n",
    "    # Sample a random action (proportions for each asset)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        logger.info(\"End of the episode reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
