{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SimpleEnv, self).__init__()\n",
    "        \n",
    "        # Define action space: The agent can choose between 0 or 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Define observation space: The state is just one number\n",
    "        self.observation_space = spaces.Box(low=0, high=10, shape=(1,))\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.state = 5  # Starting state\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = 5\n",
    "        return np.array([self.state])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (new_state, reward, done, info).\"\"\"\n",
    "        if action == 0:  # If action is 0, decrease the state\n",
    "            self.state -= 1\n",
    "        elif action == 1:  # If action is 1, increase the state\n",
    "            self.state += 1\n",
    "        \n",
    "        # Reward: High reward for being close to 10\n",
    "        reward = 10 - abs(self.state - 10)\n",
    "        \n",
    "        # Done: Episode ends if state goes out of bounds\n",
    "        done = self.state < 0 or self.state > 20\n",
    "        \n",
    "        return np.array([self.state]), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [5], Reward: 5, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [1], Reward: 1, Done: False\n",
      "State: [0], Reward: 0, Done: False\n",
      "State: [-1], Reward: -1, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Test the Simple Environment\n",
    "env = SimpleEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Choose a random action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the Portfolio Environment\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data: np.ndarray, initial_cash: float = 10000.0, transaction_fee: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize the portfolio environment.\n",
    "        :param data: Historical asset price data (numpy array).\n",
    "        :param initial_cash: Starting capital.\n",
    "        :param transaction_fee: Transaction fee percentage.\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.num_assets = data.shape[1]\n",
    "        self.transaction_fee = transaction_fee\n",
    "        \n",
    "        # Define state space and action space\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_assets + 2,))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,))\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.portfolio = np.zeros(self.num_assets)\n",
    "        logger.info(f\"Environment reset. Initial cash: {self.cash}, Portfolio: {self.portfolio}\")\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        :param action: Allocation proportions for each asset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize action to ensure it sums to 1\n",
    "        action = action / (np.sum(action) + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        logger.info(f\"Step {self.current_step}: Taking action: {[f'{x:.3f}' for x in action]}\")\n",
    "        \n",
    "        # Calculate portfolio rebalancing\n",
    "        current_prices = self.data[self.current_step]\n",
    "        logger.info(f\"current prices: {[f'{x:.3f}' for x in current_prices]}\")\n",
    "\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices) + self.cash\n",
    "        new_portfolio = portfolio_value * action / current_prices\n",
    "\n",
    "        transaction_costs = np.sum(np.abs(new_portfolio - self.portfolio)) * self.transaction_fee  # Transaction fee\n",
    "        \n",
    "        reward = portfolio_value - transaction_costs - self.cash\n",
    "        \n",
    "        # Update state\n",
    "        self.cash = portfolio_value - np.sum(new_portfolio * current_prices)\n",
    "        self.portfolio = new_portfolio\n",
    "        logger.info(f\"New portfolio: {[f'{x:.3f}' for x in self.portfolio]}, Remaining cash: {self.cash: .3f}, Reward: {reward: .3f}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Generate the current observation.\n",
    "        \"\"\"\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices)\n",
    "        return np.concatenate([current_prices, [portfolio_value, self.cash]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simulating Sample Data\n",
    "# np.random.seed(42)\n",
    "# sample_data = np.random.uniform(low=50, high=150, size=(10, 5))\n",
    "\n",
    "# # Running the environment\n",
    "# env = PortfolioEnv(data=sample_data, initial_cash=10000.0)\n",
    "# state = env.reset()\n",
    "\n",
    "# for _ in range(len(sample_data)):\n",
    "#     logging.info(\"-------------------\")\n",
    "#     # Sample a random action (proportions for each asset)\n",
    "#     action = env.action_space.sample()\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     if done:\n",
    "#         logger.info(\"End of the episode reached.\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.typing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Simulated historical price data (rows: timesteps, columns: assets)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Replace this with your actual historical data\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_system_info\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPG\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/a2c/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CnnPolicy, MlpPolicy, MultiInputPolicy\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA2C\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ClassVar, Dict, Optional, Type, TypeVar, Union\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mth\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuffers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RolloutBuffer\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gymnasium/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Root `__init__` of the gymnasium module setting the `__all__` of gymnasium modules.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# isort: skip_file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     Env,\n\u001b[1;32m      7\u001b[0m     Wrapper,\n\u001b[1;32m      8\u001b[0m     ObservationWrapper,\n\u001b[1;32m      9\u001b[0m     ActionWrapper,\n\u001b[1;32m     10\u001b[0m     RewardWrapper,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     make,\n\u001b[1;32m     15\u001b[0m     spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     register_envs,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gymnasium/core.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordConstructorArgs, seeding\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gymnasium/spaces/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This module implements various spaces.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSpaces describe mathematical sets and are used in Gym to specify valid actions and observations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mAll spaces inherit from the :class:`Space` superclass.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Box\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discrete\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gymnasium/spaces/box.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Iterable, Mapping, Sequence, SupportsFloat\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDArray\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.typing'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Simulated historical price data (rows: timesteps, columns: assets)\n",
    "# Replace this with your actual historical data\n",
    "data = np.random.uniform(low=10, high=100, size=(500, 3))  # 500 timesteps, 3 assets\n",
    "\n",
    "# Initialize the environment\n",
    "env = PortfolioEnv(data=data, initial_cash=10000.0, transaction_fee=0.001)\n",
    "\n",
    "# Wrap the environment in DummyVecEnv for compatibility with stable-baselines3\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Define the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Multi-Layer Perceptron policy\n",
    "    env,          # Portfolio environment\n",
    "    verbose=1,    # Log training process\n",
    "    tensorboard_log=\"./ppo_portfolio_tensorboard/\"  # TensorBoard log directory\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)  # Adjust timesteps based on the size of your dataset\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_portfolio_model\")\n",
    "\n",
    "# Test the model\n",
    "obs = env.reset()\n",
    "for _ in range(200):  # Test for 200 timesteps\n",
    "    action, _ = model.predict(obs)  # Use the trained model to predict actions\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
