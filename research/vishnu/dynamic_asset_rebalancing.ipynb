{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "DataFrame width maximized!\n",
      "Autoreloading of functions enabled!\n",
      "Working directory set to: /workspaces/iqinvest\n"
     ]
    }
   ],
   "source": [
    "from vpk_custom_functions import general_notebook_settings\n",
    "from vpk_custom_functions import activate_parent\n",
    "general_notebook_settings()\n",
    "activate_parent('iqinvest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SimpleEnv, self).__init__()\n",
    "        \n",
    "        # Define action space: The agent can choose between 0 or 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Define observation space: The state is just one number\n",
    "        self.observation_space = spaces.Box(low=0, high=10, shape=(1,))\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.state = 5  # Starting state\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = 5\n",
    "        return np.array([self.state])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (new_state, reward, done, info).\"\"\"\n",
    "        if action == 0:  # If action is 0, decrease the state\n",
    "            self.state -= 1\n",
    "        elif action == 1:  # If action is 1, increase the state\n",
    "            self.state += 1\n",
    "        \n",
    "        # Reward: High reward for being close to 10\n",
    "        reward = 10 - abs(self.state - 10)\n",
    "        \n",
    "        # Done: Episode ends if state goes out of bounds\n",
    "        done = self.state < 0 or self.state > 20\n",
    "        \n",
    "        return np.array([self.state]), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [5], Reward: 5, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [4], Reward: 4, Done: False\n",
      "State: [3], Reward: 3, Done: False\n",
      "State: [2], Reward: 2, Done: False\n",
      "State: [1], Reward: 1, Done: False\n",
      "State: [0], Reward: 0, Done: False\n",
      "State: [-1], Reward: -1, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Test the Simple Environment\n",
    "env = SimpleEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Choose a random action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Define the Portfolio Environment\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data: np.ndarray, initial_cash: float = 10000.0, transaction_fee: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize the portfolio environment.\n",
    "        :param data: Historical asset price data (numpy array).\n",
    "        :param initial_cash: Starting capital.\n",
    "        :param transaction_fee: Transaction fee percentage.\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.num_assets = data.shape[1]\n",
    "        self.transaction_fee = transaction_fee\n",
    "        \n",
    "        # Define state space and action space\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_assets + 2,))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,))\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.portfolio = np.zeros(self.num_assets)\n",
    "        logger.info(f\"Environment reset. Initial cash: {self.cash}, Portfolio: {self.portfolio}\")\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        :param action: Allocation proportions for each asset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize action to ensure it sums to 1\n",
    "        action = action / (np.sum(action) + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        logger.info(f\"Step {self.current_step}: Taking action: {[f'{x:.3f}' for x in action]}\")\n",
    "        \n",
    "        # Calculate portfolio rebalancing\n",
    "        current_prices = self.data[self.current_step]\n",
    "        logger.info(f\"current prices: {[f'{x:.3f}' for x in current_prices]}\")\n",
    "\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices) + self.cash\n",
    "        new_portfolio = portfolio_value * action / current_prices\n",
    "\n",
    "        transaction_costs = np.sum(np.abs(new_portfolio - self.portfolio)) * self.transaction_fee  # Transaction fee\n",
    "        \n",
    "        reward = portfolio_value - transaction_costs - self.cash\n",
    "        \n",
    "        # Update state\n",
    "        self.cash = portfolio_value - np.sum(new_portfolio * current_prices)\n",
    "        self.portfolio = new_portfolio\n",
    "        logger.info(f\"New portfolio: {[f'{x:.3f}' for x in self.portfolio]}, Remaining cash: {self.cash: .3f}, Reward: {reward: .3f}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Generate the current observation.\n",
    "        \"\"\"\n",
    "        current_prices = self.data[self.current_step]\n",
    "        portfolio_value = np.dot(self.portfolio, current_prices)\n",
    "        return np.concatenate([current_prices, [portfolio_value, self.cash]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simulating Sample Data\n",
    "# np.random.seed(42)\n",
    "# sample_data = np.random.uniform(low=50, high=150, size=(10, 5))\n",
    "\n",
    "# # Running the environment\n",
    "# env = PortfolioEnv(data=sample_data, initial_cash=10000.0)\n",
    "# state = env.reset()\n",
    "\n",
    "# for _ in range(len(sample_data)):\n",
    "#     logging.info(\"-------------------\")\n",
    "#     # Sample a random action (proportions for each asset)\n",
    "#     action = env.action_space.sample()\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     if done:\n",
    "#         logger.info(\"End of the episode reached.\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0.]\n",
      "Environment reset. Initial cash: 10000.0, Portfolio: [0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Multi-Layer Perceptron policy\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     env,          \u001b[38;5;66;03m# Portfolio environment\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,    \u001b[38;5;66;03m# Log training process\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_portfolio_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# TensorBoard log directory\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust timesteps based on the size of your dataset\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_portfolio_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:310\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    308\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 310\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:431\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_logger:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[1;32m    434\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_callback(callback, progress_bar)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/stable_baselines3/common/utils.py:201\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[0;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    198\u001b[0m save_path, format_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     latest_run_id \u001b[38;5;241m=\u001b[39m get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[0;31mImportError\u001b[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Simulated historical price data (rows: timesteps, columns: assets)\n",
    "# Replace this with your actual historical data\n",
    "data = np.random.uniform(low=10, high=100, size=(500, 3))  # 500 timesteps, 3 assets\n",
    "\n",
    "# Initialize the environment\n",
    "env = PortfolioEnv(data=data, initial_cash=10000.0, transaction_fee=0.001)\n",
    "\n",
    "# Wrap the environment in DummyVecEnv for compatibility with stable-baselines3\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Define the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Multi-Layer Perceptron policy\n",
    "    env,          # Portfolio environment\n",
    "    verbose=1,    # Log training process\n",
    "    tensorboard_log=\"./ppo_portfolio_tensorboard/\"  # TensorBoard log directory\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)  # Adjust timesteps based on the size of your dataset\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_portfolio_model\")\n",
    "\n",
    "# Test the model\n",
    "obs = env.reset()\n",
    "for _ in range(200):  # Test for 200 timesteps\n",
    "    action, _ = model.predict(obs)  # Use the trained model to predict actions\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard               2.18.0\n",
      "tensorboard-data-server   0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ppo_portfolio_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
