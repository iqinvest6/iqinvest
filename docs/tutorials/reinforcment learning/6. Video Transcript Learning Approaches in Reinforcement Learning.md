# 6\. Video Transcript: Learning Approaches in Reinforcement Learning

## Introduction to Learning Approaches

All right guys in this video we are going to look at the learning approaches in reinforcement learning. Generally, they can be classified into two categories:

1. **Policy-Based Methods**
2. **Value-Based Methods**

## Policy-Based Methods

So, these are the two main approaches that we can generalize in reinforcement learning, although sometimes some methodologies that produce very good results are a hybrid of those two methods as they use both.

### What is a Policy-Based Method?

So, remember we mentioned that the policy is much like a decision-making system. The policy-based method is a way in which an agent learns to take action, so it learns directly; it’s a direct learning method.

#### Direct Learning Method

So, we say it’s a direct learning method because the agent learns to take actions whenever it is at a certain state. For example, we have an agent that sees the price of an asset at 20K; it learns what can I do whenever I see the asset at 20K. There’s an action that it is going to take.

### Learning a Policy Function

So, we can say that since we’ve already explained that we are learning a function that maps states to actions, we are learning a policy function.

## Value-Based Methods

Now, in value-based methods, we see that policy-based methods learn directly, whereas value-based methods learn indirectly. What do we mean by that?

### Understanding Value Functions

It’s much like we learn the value of a future state and then we choose to go to that state. Let’s say, for example, we are trading, and we see that the price of an asset is 20K, and we can predict like, okay, is the next price going to be 21K or 19K?

#### Learning a Value Function

What is the value of buying the asset if it's going to 19K, or what is the value of buying the asset if it's going to 21K? So, we learn a function that tries to move the agent to a valuable state.

## Value Function Illustration

Let me put it in a much simpler way. Let’s say you live in a country and there are different cities in the country, and you really want to make a lot of money. When people want to earn a higher income, what do they do? They move to the big cities because their opportunity to earn a higher income is greater there.

#### Choosing a State Based on Value

So, the value that they want is to earn a higher income, and the big cities offer that higher value. So, they are going to learn to go to the big city. Different people value different things and so you learn this value function and you move to the state that gives you the highest value.

### Summary of Value-Based Methods

So, this is what a value-based method is trying to do: they learn a value function that represents the value of being at a certain state, and then in policy-based methods, they learn how to take the best action when they are at a certain state.

## Combining Methods

If we want to expand on this, when you move to a bigger city, there are certain actions that you still need to take, but then you know the value of being at that city. So, we can combine these two methods to form a hybrid method.

## Types of Policy-Based Methods

Let’s look at policy-based methods more closely. In policy-based methods, we have two different types of policy-based methods: **Deterministic Policy** and **Stochastic Policy**.

### Deterministic Policy

A deterministic policy states that when at a given state, the agent will always return the same action. For example, in a trading state, if the price of an asset is at 20K and we decide to buy, every time the agent encounters this price, it will buy that asset.

### Illustration of Deterministic Policy

Remember, this is a very naive example. Take it as is; it’s not something that you should practice in real life.

## Explanation of the Chart

Now, let’s look at this chart. We have a price plotted on the Y-axis and time on the X-axis. The green boxes indicate the prices at specific times. These boxes are different states. At time T equals to one, this is the state we have.

### Taking Actions in Different States

When you are in a certain state, you take an action. For example, when you see the price, what is the best action to take? You can see the actions here: green is for buy, yellow is for hold, and red is for sell.

## The Nature of Stochastic Policy

In contrast, a stochastic policy outputs the probability of actions rather than taking action directly. Instead of giving us an action, it gives the probabilities of taking various actions.

## Summary of Policy-Based Methods

We have seen the essence of deterministic and stochastic policy. In policy-based methods, the two different types help in decision-making based on state input.

## Transition to Value-Based Methods

Let’s move forward and look at another learning approach, which is value-based methods. In value-based methods, we learn a value function that maps a state to the expected value of being in that state.

### Understanding Expected Returns

The value of a state is the expected discounted return. You can calculate the value of that state based on the profit made from being at that state and following that policy to the last day.

#### Choosing the Highest Value State

According to our policy, this means that our policy is going to the state with the highest value. For example, if you’re at a certain state, you need to choose the state with the highest value to maximize profit.

## Conclusion on Learning Approaches

We have covered the two different methods of learning: **Value-Based** and **Policy-Based Methods**. If you have questions, please leave a comment, and I'll try to make it clearer or create a special video to explain the value function calculation.