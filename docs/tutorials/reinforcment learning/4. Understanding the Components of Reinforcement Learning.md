# 4\. Understanding the Components of Reinforcement Learning

## Introduction

All right guys, uh welcome to this video. where I'll be diving into a little more detail about the components of reinforcement learning.

## The Environment

### Definition of Environment

So we'll start with the environment. Like we already learned, the environment is like, uh, is the world that an agent lives in. Right? So if you are a stock trader, the stock market is your world, right? An agent that is learning how to trade uh stocks, its world is the stock market, right? Let's just put it that way.

### Representation of Environment

And this world is represented, like, you know with computers, we can only represent information as data, right? So it's represented as data.

### States and Observations

#### Understanding State

So the world of this agent is the data. And so, uh, in an environment, we've already seen that an environment will usually produce a state. A state, uh, in some definition, is a complete description of an environment, right? This doesn't apply to all applications but this term is often used in reinforcement learning.

#### Example of State

We can say that a state, so for example, if you're trading for a year and you're trading on a daily basis, so one day could be like the state, right? So your information that you have on that day is the state of that day.

#### Understanding Observation

All right, and there we have an observation. An observation is like, uh, we can define it as a partial observation of a state of an environment. So, uh, in this example, we can say that all right, if you're trading today, you have information about today, but you can never have all the information. Imagine all the news sources, all the events that are taking place in the world that lead up to what happens in the market. So all of that is like the state we can call that the state, but the observation is the prices of that stage; it's just a partial observation, right?

#### State vs. Observation in Chess

So even if we have news sources, those are partial observations. Those are like partial observations of the environment, and so we call that an observation. And so, uh, to give an example of what a state is, you know, if you play chess, at every state in a chess game, you can see every possible move that you can take. So we can say that that is a state. You can have all the information about that environment at that particular time.

#### Observation Example in Stock Market

That is what we will call a state. And to distinguish that from, um, an observation, let's say you are in a stock market; you will never have all the information about this environment at a particular time. So at a particular time, maybe you only see the price, so that's an observation. Maybe you see the price and the volume of trading; that's an observation, because you can never know all the information that is taking place at that time for you to make a decision. So we have in an environment, we have a state, and we have an observation.

## The Agent

### Definition of Agent

All right, so moving forward, we also have another component in reinforcement learning, which is an agent. So an agent is our guy; this is the guy that makes decisions for us, right?

### Role of the Agent

And this is the guy that I want to learn how to make decisions for us. So this agent, in an environment, there are certain things that an agent can do, right? And those are actions. So there are certain actions that an agent can take, and that depends on the environment and how we define what actions it can take.

### Types of Actions

#### Action Space

There are two different types of actions, right? So, uh, let's say we have an action space, which is the set of possible actions that an agent can take, right? In an environment, there are only certain things you can do. Think of it like, um, your trading today: what are the different things that you can do? You can buy, you can sell, you can buy a certain percentage of a share, or you can sell a certain percentage, right? So those are the different actions you can take.

#### Discrete vs. Continuous Actions

So we have discrete actions, which are a finite number of possible actions. We can say, okay, if you buy, sell, or hold, this is discrete: 0, 1, or negative one, right? Or continuous actions, which means you can take any action. Let's say you're buying one percent of a stock, two percent, five percent; those are continuous, right?

### Example of Actions

For example, a discrete action: you can buy, you can sell, or you can hold. We can represent it as discrete numbers: one, zero, negative one, right? Or a continuous action, where you can decide, like, “All right, what I'm going to do is I'm going to buy, uh, 50.1 percent of the stock,” or “I'm going to sell 1.1 percent of the stock,” right? So those will be continuous actions.

### Action Definition Importance

So this added in an environment, we need to define what kind of actions, right? What kind of actions, is it discrete or continuous? So whenever you go into an environment, you need to check what kind of action can this agent take; are they discrete, continuous, or are they mixed? So in some environments, you can take both discrete and continuous actions.

## The Reward

### Importance of Reward

Next, let’s look at um, the reward, which is one of the most important aspects of reinforcement learning. Like we already mentioned, it’s the central idea in reinforcement learning.

### Definition of Reward

So, what is a reward? The reward is the information that the agent gets to know how it's behaving. So if the agent is doing well, the reward is what will tell it, like, “All right boy, like these actions you're taking, like, yeah, those are the good actions.” Or if it's the actions it's taken is making you lose money, then the reward is going to be the information that you will tell the agent, like, “Bro, you need to change, you need to step up, you need to make some more money; you’re messing up,” right? So that's the reward.

### Sequential Loop in the Environment

And then, um, so in a rewarding environment. So remember we have an environment, right? So in our environment, you remember in reinforcement learning we have a sequential loop, right? So we have a sequential loop, and let's look at what a reward is, right?

### Example of Reward Calculation

So let's say we start our loop at T equals to one, which is our observation, right? So we get an observation. And then what do we do when we see that observation? We need to take an action. So let's say we take an action to buy, right? When we buy, we're going to get a reward, right? And, uh, we get a reward of zero. Why do we get this reward of zero? Because it's based on the future state.

### Understanding Future State

So we can see that, uh, the next state is still 22k, so we get a reward of zero, which means we bought under price; it didn't change, so we didn’t make money, we didn't lose money, which means the price stays the same, and we get a reward of zero, so that action wasn't bad, wasn't good.

## Continuing the Sequential Loop

And then we move to the next time step, T2. And the next time step we have the price at 22k, and then we're going to take an action. Let’s say we take the action to sell, and then we're going to get a reward. Why do we get this reward?

### Evaluating Reward After Selling

So we sold the stock and the stock moved to 21k, which means it went down, and selling means we are predicting that it's going to go down, so it did what we expected and we got a reward of plus one.

### Moving to Next Time Steps

And then we're going to go to the next state, right? We're going to go to the next time step, T3. So at T3, we buy. The stock moves to 20k. So we lost money because we bought and it went down, so we get negative one. So this loop continues to the next time step, T equals to 9.

### Terminal State Evaluation

Right? So at T equals to 9, we have 25k, our action: we buy. And what do you think is going to be a reward? Pause for a second; what is going to be a reward here? We don't know the reward. Why? Because at T equals to nine we don't know the future time step, so we cannot calculate the reward, right?

### Defining the Cumulative Reward

So we don't know if the stock went up or down, so we don't know what reward to give. So this could be like the 10. So T equals to 9 is a terminal state, right? That's the last step. And so we can see how we calculate our reward at every time step.

### Cumulative Reward Calculation

At the end of this step, we have the cumulative reward, right? So the cumulative reward will be the sum of all the rewards from all these actions, right? So, and it's going to tell us how well we perform.

### Mathematical Representation of Rewards

So, the mathematical formula to compute the cumulative reward is this. So this is the cumulative, and this is the trajectory, right? So it tells the reward at the first step plus the reward at the second step, third till the last step. Right? So we can sum it up mathematically like this.

## Discounted Rewards

### Understanding Discounted Reward

And then, uh, T is the sequence of states and actions, so just take T to be like this sequence, right? And the rewards are simply these rewards, T equals to R1, R2, R3, until the end, right?

### Explanation of Discounted Reward

And then we have. So sometimes we have a discounted reward, so the discounted reward is sometimes we give a discount to some rewards. I will explain this in detail in a future lecture, but now, um, so the discounted reward, we have a discount rate which tells us how much discount we need to give a reward, right?

### Final Reward with Discounting

So, and so the final, uh, if we use discounted rewards, our final reward is going to be the discounted cumulative reward. So this is just the cumulative reward with no discount.

### More on Discounted Cumulative Rewards

So discounted cumulative reward means we've added a discount rate for our rewards, right? We're going to explain that in further detail in a future video.

## Summary of Components

### Recap of Key Components

And so, uh, still looking at explaining what the reward is, like we've seen what the discounted cumulative reward is and what the accumulative reward is.

### Environmental Dynamics

So we can see, like, um, let's say this is our environment; it gives us the sequence. We have our sequential loops: state, action, reward, and future state. We get these loops.

### Cumulative and Discounted Rewards

And so at time step one, let's say our discount is some value, right? This discount is, uh, calculated from the formula you can see. At T1, it's the discount rate raised to the power of zero. At T9, the discount rate raised to the power of eight, right? It's from the mathematical formula that we can see here, right?

### Reward Calculation Example

And then we have the rewards. So for example, for our sequence, you can see the rewards here; these are the rewards that the agent got. So if we calculate that, we can use this to calculate the cumulative reward.

### Final Thoughts

So what's going to be the cumulative reward, right? So if you calculate the cumulative reward, we’ll see that our agent actually lost one unit, right, from our bet. So if that's the summation, right?

### Alternative Discount Rate

And then if we look at the discounted reward, let’s say we multiply it, let’s say our discount is equal to one, and we multiply that by the reward, right, to get the discounted rewards. So we get the discounted reward for every time step.

### Conclusion

And then we sum them up to get the discounted cumulative reward. So if we do that, let’s say our discount is one, which means we multiply every reward by just one; we don’t want to make that complex, but there are situations where we need to get a different value that will work better to inform our agent. So, we get a discounted cumulative reward of negative one; it's negative one because we choose the discount to be one, which means there’s no discount. Right? Yeah.

And so this is, um, how we can explain the components of reinforcement learning. So we've seen, uh, the environment that produces states or observations, and then we've seen the different actions which are either discrete or continuous, and we've seen the reward which can be accumulative or discounted cumulative rewards. So these are the fundamental components of reinforcement learning, and I hope you've learned something. See you in the next video!